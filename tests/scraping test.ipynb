{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import unicodedata\n",
    "from random import randint\n",
    "from time import sleep\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Função figure_out_target_pages( ) considera o funcionamento da página de resultados do site Reclame Aqui \n",
    "e calcula os números corretos de páginas a serem inseridas no URL que contém um \n",
    "conjunto de 10 novas reclamações, evitando a coleta repetida.\n",
    "\n",
    "Recebe como argumento o número de reclamações máximo que se deseja coletar,\n",
    "a partir do qual calcula as páginas corretas para raspagem. Suporta até o \n",
    "número máximo de reclamações registradas no perfil, que devem ser visualizadas\n",
    "no site.\n",
    "\n",
    "Parâmetros:\n",
    "n_of_complaints (int): indica o número de reclamações que se deseja coletar do site\n",
    "\n",
    "'''\n",
    "\n",
    "def figure_out_target_pages(n_of_complaints):   \n",
    "  n_of_pages = math.floor(n_of_complaints / 10)+1\n",
    "  true_new_complaints_pages = []\n",
    "  for i in range(0,n_of_pages):\n",
    "    x = i*10+1\n",
    "    true_new_complaints_pages.append(x)\n",
    "  # retorna lista com os números de páginas corretos para raspagem\n",
    "  return true_new_complaints_pages  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Função scrape_search_results_RA ( ) recebe nome da empresa conforme padrão registrado no URL da sua página de perfil\n",
    "e número de reclamações a serem coletadas. Chama a função figure_out_target_pages( ) e itera sobre a lista\n",
    "resultante coletando apenas títulos, datas e links da página de resultados. Retorna um dicionário RA_dict \n",
    "com os dados coletados da página de busca.\n",
    "\n",
    "Parâmetros:\n",
    "company_name_url (str): string com o nome da empresa tal como escrito na url da página de perfil da empresa no site\n",
    "n_of_complaints  (int): indica o número de reclamações que se deseja coletar do site\n",
    "\n",
    "'''\n",
    "\n",
    "def scrape_search_results_RA (company_name_url, n_of_complaints):\n",
    "  RA_dict = {\n",
    "             'Titles' : [],\n",
    "             'Dates'  : [],\n",
    "             'Links'  : []\n",
    "            }\n",
    "\n",
    "  titles  = []\n",
    "  dates   = []\n",
    "  links   = []\n",
    "  \n",
    "  target_pages = figure_out_target_pages(n_of_complaints)\n",
    "  for page in target_pages:\n",
    "    delay = randint(0,5)\n",
    "    print(f'Cool down of {delay}.\\nScraping page number {page}')\n",
    "\n",
    "    # Acessa página da empresa e itera sobre os números em target_pages\n",
    "    # Informa informações de headers para conseguir acesso ao site\n",
    "    # Gera doc html para raspagem\n",
    "    url      = f'https://www.reclameaqui.com.br/empresa/{company_name_url}/lista-reclamacoes/?pagina={page}'\n",
    "    headers  = {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/116.0'}\n",
    "    response = requests.get(url, headers = headers)\n",
    "    html_doc = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Testa response e em caso de erro, imprime erro e encerra loop\n",
    "    if str(response) != '<Response [200]>':\n",
    "      print(url); print(f'Erro de Request!\\n{response}\\n Entre em contato se precisa de ajuda.')\n",
    "      break\n",
    "\n",
    "    # Em caso de erro inexistente, inicia raspagem\n",
    "    else:\n",
    "      cards = html_doc.find_all('div', class_ = 'sc-1pe7b5t-0 iQGzPh')\n",
    "      for card in cards:\n",
    "        title = card.find('h4')\n",
    "        titles.append(title.text)\n",
    "        date = card.find('span', class_ = 'sc-1pe7b5t-5 bmtSzo')\n",
    "        dates.append(date.text)\n",
    "        link = card.find('a')\n",
    "        links.append(f'https://www.reclameaqui.com.br{link[\"href\"]}')\n",
    "\n",
    "  # Verifica lengths das listas para evitar erros na criação do data frame\n",
    "  a = len(titles)\n",
    "  print(f'Length of titles = {a}')\n",
    "  list(titles)\n",
    "  b = len(dates)\n",
    "  print(f'Length of dates = {b}')\n",
    "  list(dates)\n",
    "  c = len(links)\n",
    "  print(f'Length of links = {b}')\n",
    "  list(links)\n",
    "  if a == b and a == c:\n",
    "    print(f'Listas de dados raspados com o mesmo tamanho. {a} items.')\n",
    "  else :\n",
    "    print('Error!\\nListas de dados raspados com tamanhos diferentes.\\nEntre em contato se precisa de ajuda')\n",
    "\n",
    "  # Atualiza dicionário com dados raspados e retorna dict\n",
    "  RA_dict['Titles'] = titles\n",
    "  RA_dict['Dates']  = dates\n",
    "  RA_dict['Links']  = links\n",
    "  return RA_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Função scrape_complaints_RA ( ) recebe dicionário gerado por scrape_search_results_RA( ), \n",
    "acessa links coletados e raspa texto completo de reclamação do OP e mensagem \n",
    "subsequente, podendo esta ser uma réplica, segundo comentário do OP ou nenhuma,\n",
    "registrando a ausência de mensagem subsequente como 'Não respondida'.\n",
    "\n",
    "Parâmetros:\n",
    "RA_dict (dict): recebe a variável que armazena o dicionário produzido pela função scrape_search_results_RA( )\n",
    "\n",
    "'''\n",
    "\n",
    "def scrape_complaints_RA(RA_dict):\n",
    "  links = RA_dict['Links']\n",
    "  complaints = []\n",
    "  answers    = []\n",
    "\n",
    "  for link in links:\n",
    "    delay = randint(0,5)\n",
    "    print(f'Cool down of {delay}.\\nScraping {link}')\n",
    "    sleep(delay)\n",
    "\n",
    "    # Acessa links, informa headers para receber acesso ao site e gera doc html para raspagem\n",
    "    url      = link\n",
    "    headers  = {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/116.0'}\n",
    "    response = requests.get(url, headers = headers)\n",
    "    html_doc = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Testa response, imprime erro e encerra loop ou procede com a raspagem\n",
    "    if str(response) != '<Response [200]>':\n",
    "      print(url); print(f'Erro de Request!\\n{response}\\n Entre em contato se precisa de ajuda.')\n",
    "      break\n",
    "    else:\n",
    "      complaint_text = html_doc.find('p', class_ = 'lzlu7c-17 cNqaUv').text\n",
    "      complaints.append(complaint_text)\n",
    "\n",
    "      # verifica existência de texto de resposta, em caso de inexistência adiciona \"Não Respondida\" à lista answers\n",
    "      answer = html_doc.find('p', class_ = 'sc-1o3atjt-4 kBLLZs')\n",
    "      if answer == None:\n",
    "        answers.append('Não respondida')\n",
    "      else:\n",
    "        answers.append(answer.text)\n",
    "\n",
    "  # Atualiza dicionário com dados raspados\n",
    "  RA_dict['Complaints'] = complaints\n",
    "  RA_dict['Answers']    = answers\n",
    "  return RA_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Função final para o usuário scrape_RA( ), recebe input do usuário, executa as funções\n",
    "scrape_search_results_RA( ) e scrape_complaints_RA( ), cria df a partir do RA_dict \n",
    "gerado por scrape_complaints_RA( ) e salva arquivo no disco ou Google Drive do usuário.\n",
    "\n",
    "Parâmetros:\n",
    "company_name_url (str): string com o nome da empresa tal como escrito na url da página de perfil da empresa no site\n",
    "n_of_complaints  (int): indica o número de reclamações que se deseja coletar do site\n",
    "file_type        (str): aceita \"excel\", \"csv\" ou \"csv + excel\", indicando formato em que se deseja exportar o df gerado\n",
    "save_path        (str): indica o diretório em que se deseja salvar o arquivo\n",
    "\n",
    "'''\n",
    "\n",
    "def scrape_RA(company_url, n_of_complaints, file_type, save_path):\n",
    "  company_name_url = company_url.replace('https://www.reclameaqui.com.br/empresa/', '').replace('lista-reclamacoes/', '').replace('/','')\n",
    "  n_of_complaints  = int(n_of_complaints)\n",
    "  RA_dict          = scrape_search_results_RA(company_name_url, n_of_complaints)\n",
    "  RA_dict          = scrape_complaints_RA(RA_dict)\n",
    "  RA_df            = pd.DataFrame.from_dict(RA_dict)\n",
    "\n",
    "  if file_type   == 'excel':\n",
    "    RA_df.to_excel(f'{save_path}\\\\ReclameAqui_{company_name_url}.xlsx', index=False)\n",
    "  elif file_type == 'csv':\n",
    "    RA_df.to_csv(f'{save_path}\\\\ReclameAqui_{company_name_url}.csv', index=False)\n",
    "  elif file_type == 'csv + excel':\n",
    "    RA_df.to_excel(f'{save_path}\\\\ReclameAqui_{company_name_url}.xlsx', index=False)\n",
    "    RA_df.to_csv(f'{save_path}\\\\ReclameAqui_{company_name_url}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cool down of 5.\n",
      "Scraping page number 1\n",
      "Cool down of 0.\n",
      "Scraping page number 11\n",
      "Cool down of 2.\n",
      "Scraping page number 21\n",
      "Cool down of 4.\n",
      "Scraping page number 31\n",
      "Cool down of 3.\n",
      "Scraping page number 41\n",
      "Cool down of 5.\n",
      "Scraping page number 51\n",
      "Cool down of 3.\n",
      "Scraping page number 61\n",
      "Cool down of 2.\n",
      "Scraping page number 71\n",
      "Cool down of 0.\n",
      "Scraping page number 81\n",
      "Cool down of 3.\n",
      "Scraping page number 91\n",
      "Cool down of 3.\n",
      "Scraping page number 101\n",
      "Length of titles = 50\n",
      "Length of dates = 50\n",
      "Length of links = 50\n",
      "Listas de dados raspados com o mesmo tamanho. 50 items.\n",
      "Cool down of 1.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/cadastro-bloqueado_7G3mQPQpsmAjuSf1/\n",
      "Cool down of 2.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/ativacao-de-cadastro_m1Bbbe2G9XZCqP-S/\n",
      "Cool down of 3.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/entrada-proibida_3_NVNdt51NSAbeI4/\n",
      "Cool down of 3.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/cupom-nao-esta-funcionando_wgV_1O6FIXtdM8Nf/\n",
      "Cool down of 0.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/cupom-nao-funciona_awk4_KBtOwcPxoG5/\n",
      "Cool down of 4.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/reembolso-de-compras_L_nwkgziP_eDy-oT/\n",
      "Cool down of 5.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/mais-uma-vez-impedido-de-comprar_VuKAhT7WVcQ7D0u8/\n",
      "Cool down of 1.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/nao-consigo-utilizar-o-cupom_kxhegvlQ5eXUdFpn/\n",
      "Cool down of 5.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/cupom_icY7YbS4A3btgLlN/\n",
      "Cool down of 5.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/falta-de-atendimento-rapido_wm1CRFGbzxRlexqv/\n",
      "Cool down of 2.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/impossivel-fazer-qualquer-m3-da-nesse-site-totalmente-bugado_lYI6DLujtlBmOprb/\n",
      "Cool down of 3.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/ingresso-perdido_FHdUl00EGqYyyzD5/\n",
      "Cool down of 1.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/nao-consigo-atendimento_qielf4_OayPLvF1a/\n",
      "Cool down of 2.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/nao-consigo-comprar-ingressos_4QC0pRmNOpqn-oSv/\n",
      "Cool down of 3.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/compra-nao-concluida_GTYhxI4gX_iLUdXZ/\n",
      "Cool down of 0.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/compra-indevida-utilizando-meu-cartao_GsZq2s7G6CulD9dV/\n",
      "Cool down of 2.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/kinopass-cancelamento_-CWypBoiIhQohAhA/\n",
      "Cool down of 5.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/reembolso_LE_NECQaZlDFWsQr/\n",
      "Cool down of 3.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/nao-consigo-falar-com-humano_hDgk2Zf_N9zaohoe/\n",
      "Cool down of 2.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/compra-com-erro_EM-m0jzx-itZ6fTL/\n",
      "Cool down of 4.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/nao-consigo-usar-cupom-no-cinema_xxtOSscmwG_qZpmT/\n",
      "Cool down of 0.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/nao-recebi-os-ingressos-por-e-mail_yBMWJ9SfrFlqVFTl/\n",
      "Cool down of 5.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/compra-de-ingresso-com-data-errada_bTeOuFeLtSm_cF52/\n",
      "Cool down of 5.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/falta-de-informacao_jCcM1HOEkovFU6ah/\n",
      "Cool down of 3.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/mentiras-de-funcionarios-e-estou-sendo-prejudicado-por-cobranca-indevida-po_7BcFgJmL7T_IpLbS/\n",
      "Cool down of 3.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/nao-resolveram-meu-problema_lzvaoCM28iwJeILR/\n",
      "Cool down of 3.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/cancelamento-de-compra-de-ingresso_GOH9e58hx5Pns41J/\n",
      "Cool down of 0.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/pessima-experiencia-com-o-site-recriar-senha_BGn61axObEC8Xsz7/\n",
      "Cool down of 4.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/nao-consigo-fazer-login_Ty9w3iZhC5vnf6tl/\n",
      "Cool down of 4.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/ingresso_AYrZNoFE7LgWHprC/\n",
      "Cool down of 5.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/nunca-consigo-comprar-ingresso_X53JM-cyFk3-C8te/\n",
      "Cool down of 5.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/nao-consigo-comprar-ingressos-pro-cine-araujo_mmAAx-E5CAzn7oof/\n",
      "Cool down of 1.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/reembolso_3iRJ9V5nzucdQ_Au/\n",
      "Cool down of 2.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/erro-na-compra-de-ingresso_OcZbIqjmM7gSJ-IB/\n",
      "Cool down of 4.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/nao-obtive-respostas-para-o-meu-cancelamento_qCZ5NUx65Y0p6lqL/\n",
      "Cool down of 4.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/cobranca-indevida_7M-hvIJOND6yG6M8/\n",
      "Cool down of 1.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/erro-na-hora-de-finalizar-a-compra_XJy9Y6v3SnKaN1wi/\n",
      "Cool down of 1.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/erro-ao-concluir-o-pagamento_YGNHCfI1suYCp9Gj/\n",
      "Cool down of 2.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/erro-ao-tentar-utilizar-o-cupom_Lvd-1cwgOjShqItm/\n",
      "Cool down of 2.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/erro-na-plataforma-e-descaso-no-suporte_LaI09Reqn1HJjStC/\n",
      "Cool down of 3.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/falta-de-informacao_HMB95XHvuLDV3tRr/\n",
      "Cool down of 5.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/ingresso-comprado-com-horario-em-que-o-filme-ja-estava-rodando_J5txyNS9qbZ-BZde/\n",
      "Cool down of 5.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/o-pior-servico-do-momento_RMY56p7ho-YibQAk/\n",
      "Cool down of 2.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/tickets-com-data-errada_anxKP0qaot3H1f7O/\n",
      "Cool down of 0.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/barrada-no-cinema_RCAfpidvTf4B9lll/\n",
      "Cool down of 4.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/solicito-uma-acao-rapida_pMpX3Dmpb7YJ0sYu/\n",
      "Cool down of 2.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/fiz-uma-compra-e-queria-pedir-o-estorno-do-valor-mas-nao-consegui_nB3FCEShz8UaQmCP/\n",
      "Cool down of 1.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/nao-consigo-concluir-a-compra_Dylv3K332W13o0wZ/\n",
      "Cool down of 3.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/politica-cancelamento-nao-estornado_5AWmD8_tJrpNSZ-8/\n",
      "Cool down of 5.\n",
      "Scraping https://www.reclameaqui.com.br/ingresso-com/nao-consigo-finalizar-compra_ogvdmPfft1cG1KOX/\n"
     ]
    }
   ],
   "source": [
    "\"https://www.reclameaqui.com.br/empresa/magazine-luiza-loja-fisica/\"\n",
    "\n",
    "# Altere os valores abaixo conforme as instruções acima:\n",
    "company_url = \"https://www.reclameaqui.com.br/empresa/ingresso-com/\" # não esqueça as aspas!\n",
    "n_of_complaints = \"100\"\n",
    "file_type = \"excel\"\n",
    "save_path = \"C:\\\\Users\\\\João Flavio\\\\Desktop\\\\Code\\\\Scraping Reclame Aqui\\\\tests\\\\results\"\n",
    "\n",
    "# Não alterar código abaixo!\n",
    "scrape_RA(company_url, n_of_complaints, file_type, save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
