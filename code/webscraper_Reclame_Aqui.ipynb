{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import unicodedata\n",
    "from random import randint\n",
    "from time import sleep\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Função figure_out_target_pages( ) considera o funcionamento da página de resultados do site Reclame Aqui \n",
    "e calcula os números corretos de páginas a serem inseridas no URL que contém um \n",
    "conjunto de 10 novas reclamações, evitando a coleta repetida.\n",
    "\n",
    "Recebe como argumento o número de reclamações máximo que se deseja coletar,\n",
    "a partir do qual calcula as páginas corretas para raspagem. Suporta até o \n",
    "número máximo de reclamações registradas no perfil, que devem ser visualizadas\n",
    "no site.\n",
    "\n",
    "Parâmetros:\n",
    "n_of_complaints (int): indica o número de reclamações que se deseja coletar do site\n",
    "\n",
    "'''\n",
    "\n",
    "def figure_out_target_pages(n_of_complaints):   \n",
    "  n_of_pages = math.floor(n_of_complaints / 10)+1\n",
    "  true_new_complaints_pages = []\n",
    "  for i in range(0,n_of_pages):\n",
    "    x = i*10+1\n",
    "    true_new_complaints_pages.append(x)\n",
    "  # retorna lista com os números de páginas corretos para raspagem\n",
    "  return true_new_complaints_pages  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Função scrape_search_results_RA ( ) recebe nome da empresa conforme padrão registrado no URL da sua página de perfil\n",
    "e número de reclamações a serem coletadas. Chama a função figure_out_target_pages( ) e itera sobre a lista\n",
    "resultante coletando apenas títulos, datas e links da página de resultados. Retorna um dicionário RA_dict \n",
    "com os dados coletados da página de busca.\n",
    "\n",
    "Parâmetros:\n",
    "company_name_url (str): string com o nome da empresa tal como escrito na url da página de perfil da empresa no site\n",
    "n_of_complaints  (int): indica o número de reclamações que se deseja coletar do site\n",
    "\n",
    "'''\n",
    "\n",
    "def scrape_search_results_RA (company_name_url, n_of_complaints):\n",
    "  RA_dict = {\n",
    "             'Titles' : [],\n",
    "             'Dates'  : [],\n",
    "             'Links'  : []\n",
    "            }\n",
    "\n",
    "  titles  = []\n",
    "  dates   = []\n",
    "  links   = []\n",
    "  \n",
    "  target_pages = figure_out_target_pages(n_of_complaints)\n",
    "  for page in target_pages:\n",
    "    delay = randint(0,5)\n",
    "    print(f'Cool down of {delay}.\\nScraping page number {page}')\n",
    "\n",
    "    # Acessa página da empresa e itera sobre os números em target_pages\n",
    "    # Informa informações de headers para conseguir acesso ao site\n",
    "    # Gera doc html para raspagem\n",
    "    url      = f'https://www.reclameaqui.com.br/empresa/{company_name_url}/lista-reclamacoes/?pagina={page}'\n",
    "    headers  = {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/116.0'}\n",
    "    response = requests.get(url, headers = headers)\n",
    "    html_doc = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Testa response e em caso de erro, imprime erro e encerra loop\n",
    "    if str(response) != '<Response [200]>':\n",
    "      print(url); print(f'Erro de Request!\\n{response}\\n Entre em contato se precisa de ajuda.')\n",
    "      break\n",
    "\n",
    "    # Em caso de erro inexistente, inicia raspagem\n",
    "    else:\n",
    "      cards = html_doc.find_all('div', class_ = 'sc-1pe7b5t-0 iQGzPh')\n",
    "      for card in cards:\n",
    "        title = card.find('h4')\n",
    "        titles.append(title.text)\n",
    "        date = card.find('span', class_ = 'sc-1pe7b5t-5 bmtSzo')\n",
    "        dates.append(date.text)\n",
    "        link = card.find('a')\n",
    "        links.append(f'https://www.reclameaqui.com.br{link[\"href\"]}')\n",
    "\n",
    "  # Verifica lengths das listas para evitar erros na criação do data frame\n",
    "  a = len(titles)\n",
    "  print(f'Length of titles = {a}')\n",
    "  list(titles)\n",
    "  b = len(dates)\n",
    "  print(f'Length of dates = {b}')\n",
    "  list(dates)\n",
    "  c = len(links)\n",
    "  print(f'Length of links = {b}')\n",
    "  list(links)\n",
    "  if a == b and a == c:\n",
    "    print(f'Listas de dados raspados com o mesmo tamanho. {a} items.')\n",
    "  else :\n",
    "    print('Error!\\nListas de dados raspados com tamanhos diferentes.\\nEntre em contato se precisa de ajuda')\n",
    "\n",
    "  # Atualiza dicionário com dados raspados e retorna dict\n",
    "  RA_dict['Titles'] = titles\n",
    "  RA_dict['Dates']  = dates\n",
    "  RA_dict['Links']  = links\n",
    "  return RA_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Função scrape_complaints_RA ( ) recebe dicionário gerado por scrape_search_results_RA( ), \n",
    "acessa links coletados e raspa texto completo de reclamação do OP e mensagem \n",
    "subsequente, podendo esta ser uma réplica, segundo comentário do OP ou nenhuma,\n",
    "registrando a ausência de mensagem subsequente como 'Não respondida'.\n",
    "\n",
    "Parâmetros:\n",
    "RA_dict (dict): recebe a variável que armazena o dicionário produzido pela função scrape_search_results_RA( )\n",
    "\n",
    "'''\n",
    "\n",
    "def scrape_complaints_RA(RA_dict):\n",
    "  links = RA_dict['Links']\n",
    "  complaints = []\n",
    "  answers    = []\n",
    "\n",
    "  for link in links:\n",
    "    delay = randint(0,5)\n",
    "    print(f'Cool down of {delay}.\\nScraping {link}')\n",
    "    sleep(delay)\n",
    "\n",
    "    # Acessa links, informa headers para receber acesso ao site e gera doc html para raspagem\n",
    "    url      = link\n",
    "    headers  = {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/116.0'}\n",
    "    response = requests.get(url, headers = headers)\n",
    "    html_doc = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Testa response, imprime erro e encerra loop ou procede com a raspagem\n",
    "    if str(response) != '<Response [200]>':\n",
    "      print(url); print(f'Erro de Request!\\n{response}\\n Entre em contato se precisa de ajuda.')\n",
    "      break\n",
    "    else:\n",
    "      complaint_text = html_doc.find('p', class_ = 'lzlu7c-17 cNqaUv').text\n",
    "      complaints.append(complaint_text)\n",
    "\n",
    "      # verifica existência de texto de resposta, em caso de inexistência adiciona \"Não Respondida\" à lista answers\n",
    "      answer = html_doc.find('p', class_ = 'sc-1o3atjt-4 kBLLZs')\n",
    "      if answer == None:\n",
    "        answers.append('Não respondida')\n",
    "      else:\n",
    "        answers.append(answer.text)\n",
    "\n",
    "  # Atualiza dicionário com dados raspados\n",
    "  RA_dict['Complaints'] = complaints\n",
    "  RA_dict['Answers']    = answers\n",
    "  return RA_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Função final para o usuário scrape_RA( ), recebe input do usuário, executa as funções\n",
    "scrape_search_results_RA( ) e scrape_complaints_RA( ), cria df a partir do RA_dict \n",
    "gerado por scrape_complaints_RA( ) e salva arquivo no disco ou Google Drive do usuário.\n",
    "\n",
    "Parâmetros:\n",
    "company_name_url (str): string com o nome da empresa tal como escrito na url da página de perfil da empresa no site\n",
    "n_of_complaints  (int): indica o número de reclamações que se deseja coletar do site\n",
    "file_type        (str): aceita \"excel\", \"csv\" ou \"csv + excel\", indicando formato em que se deseja exportar o df gerado\n",
    "save_path        (str): indica o diretório em que se deseja salvar o arquivo\n",
    "\n",
    "'''\n",
    "\n",
    "def scrape_RA(company_url, n_of_complaints, file_type, save_path):\n",
    "  company_name_url = company_url.replace('https://www.reclameaqui.com.br/empresa/', '').replace('lista-reclamacoes/', '').replace('/','')\n",
    "  n_of_complaints  = int(n_of_complaints)\n",
    "  RA_dict          = scrape_search_results_RA(company_name_url, n_of_complaints)\n",
    "  RA_dict          = scrape_complaints_RA(RA_dict)\n",
    "  RA_df            = pd.DataFrame.from_dict(RA_dict)\n",
    "\n",
    "  if file_type   == 'excel':\n",
    "    RA_df.to_excel(f'{save_path}\\\\ReclameAqui_{company_name_url}.xlsx', index=False)\n",
    "  elif file_type == 'csv':\n",
    "    RA_df.to_csv(f'{save_path}\\\\ReclameAqui_{company_name_url}.csv', index=False)\n",
    "  elif file_type == 'csv + excel':\n",
    "    RA_df.to_excel(f'{save_path}\\\\ReclameAqui_{company_name_url}.xlsx', index=False)\n",
    "    RA_df.to_csv(f'{save_path}\\\\ReclameAqui_{company_name_url}.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
